\chapter{Revisão Bibliográfica}
\label{cap:revisao}

\subsection{Objetivo}
	O objetivo dessa revisão foi analisar e estudar as maneiras já existentes de otimizar o desempenho da rede em ambientes de máquinas virtuais
 e os problemas em aberto que esse tipo de rede apresenta. Cada estratégia sugerida pode atender bem a um cenário, porém, em outros casos, essa mesma estratégia pode ser pouco eficiente devido a dinamicidade da rede e os diferentes requerimentos de um usuário.

\subsection{Busca}
	Para busca de referências foram usados o mecanismo de busca do google Acadêmico e  algumas pessoas especializadas na área. Apenas artigos em inglês e português foram selecionados.

\subsection{Palavra-chave}
	TODO

\subsection{Critérios de Seleção}
Para seleção das referências, em cada artigo encontrado pela estrategia de busca será lido o seu resumo e classificado manualmente em três categorias de acordo com sua relevância: alta, média, baixa.
Os artigos de relevância alta serão lidos por completo e resumidos. Os artigos médios terão a leitura de sua introdução e a mudança da sua relevância para baixa ou alta. Por fim os artigos baixos não serão lidos. 


\subsection{Execução}
{A tabela \ref{artigos} mostra os artigos coletados e sua relevância.

{
\begin{table}[ht]
\caption{Artigos selecionados e suas relevâncias}
\label{artigos}
\begin{center}
	\begin{tabular}{| cp{2cm} | r}
		artigo&&relevância\\
	\hline
		\cite{chaudhary2008comparison}&&alta\\
		\cite{ekanayake2010high}&&alta\\
		\cite{Waldspurger:2012:IV:2063176.2063194}&&alta\\
		\cite{liu2010evaluating}&&alta\\
		\cite{Rixner:2008:NVB:1348583.1348592}&&alta\\
		\cite{shipman07:_inves_infin}&&média-baixa\\
		\cite{oi2009performance}&&alta\\
		\cite{gro}&&alta\\
	\end{tabular}
\end{center}
\end{table}
}
}

\subsection{Resumo Sintetizado}
Em \cite{chaudhary2008comparison} foi feito uma comparação entre diferentes tecnologias de virtualização.
Nos experimentos foi concluído que o \textit{hypervisor} XEN tem um desempenho baixo em termos de atraso na rede, porém alto em termos de largura de banda em relação a um ambiente com OpenVZ e um ambiente sem virtualização, enquanto que o OpenVZ tem uma perda considerável em largura de banda, mas um atraso pequeno. Os autores de \cite{chaudhary2008comparison} não entram em detalhes sobre os motivos dos resultados terem sido esses.

 \cite{ekanayake2010high} estudou a relação entre o número de núcleos e o número de MVs usando XEN e Eucalyptus como infraestrutura de nuvem. Foi concluído que a virtualização funciona bem para aplicações que não se não se comunicam muito,
 enquanto que em aplicações que são sensíveis a latência, houve uma perda de desempenho em relação a um ambiente não virtualizado. Outra conclusão foi que quanto maior o número de máquinas virtuais, maior a sobrecarga na CPU. A explicação para isso, segundo o autor, está na forma como foi implementada a rede virtual. O hardware físico só pode ser controlado por um sistema (\texttt{dom0}), enquanto que os outros (\texttt{domUs}) para conseguir fazer alguma operação de E/S pela rede, tem que passar por esse sistema através de um canal. Isso forma um gargalo no \texttt{dom0}.


\cite{Rixner:2008:NVB:1348583.1348592} fez uma revisão sobre a virtualização de rede.
No texto o autor cita que a virtualização de rede impacta diretamente no número de servidores que podem ser diretamente consolidados dentro de uma única máquina física. Porém, as técnicas modernas de virtualização têm gargalos significantes, o que limita o desempenho da rede.

Ele sugere um ganho de desempenho fazendo o dispositivo ter a capacidade de ler e escrever diretamente na memória da MV ao invés do processador da máquina virtual gerar interrupções cada vez que alguma informação entra ou sai pelo dispositivo. Essa funcionalidade é chamada acesso direto a memória (\textit{DMA}).

Apesar disso, o dispositivo pode escrever em uma posição da memória que não pertence a MV e, podendo assim, causar problemas em outros processos da máquina física. Assim, foi criado a unidade de gerenciamento de E/S da memória (\textit{IOMMU}). No \textit{IOMMU} a memória é restrita para o dispositivo de acordo com a máquina virtual que controla esse dispositivo.

Na virtualização de E/S compartilhada, cada dispositivo de E/S pode ser associado a várias MVs, cada máquina tem acesso ao dispositivo dependendo da informação.

Como atualmente um processador possui vários núcleos, podemos aproveitar esses núcleos para criar multi-filas nas interfaces de rede. Numa pesquisa da HP foi estudado a remoção da ponte no domínio de E/S para associar as filas diretamente com o driver de \textit{backend}, evitando a necessidade de sincronização das mensagens e multiplexação/demultiplexação da rede. Como benefícios do uso da multi-fila temos: a redução da carga extra na fila e a eliminação de cópias entre o domínio de E/S e a máquina virtual, pois, a multiplexação não é feita. Por outro lado, é necessário que cada informação seja mandada para a fila correta e que a CPU consiga aguentar a carga extra gerada pelas múltiplas filas.

Na arquitetura de rede virtual CDNA (\textit{acesso direto a memória concorrente}) foi usado a ideia de multi-filas e em adição removeram o dominio de E/S. Sem o responsável por controlar as filas, o \textit{hypervisor} passa a considerar cada conjunto de fila como um interface de rede física e a associa o controlador a uma MV. Assim, cada MV consegue enviar ou receber informações diretamente da rede sem nenhuma interversão do domínio de E/S. Como consequência, a carga extra é reduzida pelo número reduzido de interrupções (antes era necessário interromper tanto o domínio de E/S como as MVs em cada transmissão/recepção).

Pela MV poder acessar diretamente a interface de rede, ela também pode acessar algum local indevido da memória por \textit{DMA}. Para contornar esse problema o autor sugeriu o uso de \textit{IOMMU}.

\cite{Waldspurger:2012:IV:2063176.2063194}  cita diversos desafios e problemas na área de virtualização de E/S como: a carga extra no \textit{hypervisor}, a complexidade em gerenciar recursos (escalonamento e prioridades) e a dificuldade de dar uma semântica ao hardware virtual.

Quando usamos esses dispositivos, junto com a funcionalidade de DMA(acesso direto a memória), a MV pode acessar uma área que não pertence a ela. Uma possível solução é usar\textit{IOMMU} onde é feito um mapeamento entre a MV e a memória e impede uma MV de acessar um pedaço da memória que não lhe pertence.
 
\cite{liu2010evaluating} fez diversos experimentos com virtualizações baseados em \textit{software} (\textit{virtio}) e em hardware (\textit{SR-IOV}) usando o \textit{hypervisor} KVM. Foram analisadas diversas métricas: a largura de banda, a latência, chamadas para memória, comunicação entre máquinas, tamanho da MTU, IRQ afinitivo e distribuído e \textit{IOMMU}.

 Na latência, o \textit{virtio} (driver para virtualizar a E/S nas MVs) teve um desempenho muito baixo. A explicação, provada desabilitando a função de transmissão em lote, é que o hospedeiro atrasa o pacote para ser mandado em lotes, mas mesmo assim, seu desempenho sem espera ainda perdeu próximo de 20 microssegundo em relação a máquina pura. Por outro lado, desabilitar essa função provocaria uma perca de desempenho com quantidade grande de carga de trabalho. Já o \textit{SR-IOV} teve um desempenho próximo da máquina pura perdendo apenas alguns microssegundos devido a virtualização da interrupção.

 Na largura de banda, a transmissão em todos pareceu ter o mesmo desempenho. Já na recepção o \textit{SR-IOV}(single root I/O virtualization) se aproximou da máquina pura, mas o uso da sua CPU foi muito maior que as demais. No \textit{virtio}, ele não conseguiu um bom desempenho, mas o uso de sua CPU foi baixa. No experimento de uso da memória na recepção, o \textit{SR-IOV} teve um uso muito menor que o \textit{virtio}, assim, o autor concluiu que o mal uso da largura de banda na recepção do virtio foi pelo uso excessivo da memória, o que explica também o baixo uso da CPU.

Nas transmissões de dados de MV, o \textit{SR-IOV} teve uma quantidade muito maior que o \textit{virtio}. A explicação do autor está na forma com que o \textit{virtio} age quando tem uma alta carga na recepção, durante esse momento, ele troca a interrupção para um modo de pesquisa que reduz o número de interrupções e melhora o desempenho da rede.
Na separação de custos, foi feita a separação do quanto foi utilizado de CPU para determinada tarefa. O resultado foi que o \textit{virtio} gasta uma quantidade maior na virtualização e cópia, enquanto que o \textit{SR-IOV} gasta uma quantidade maior em interrupções e nas máquinas virtuais.


\cite{Santos:2008:BGS:1404014.1404017} propôs modificar a arquitetura do \textit{driver} de E/S do \textit{XEN} para conseguir melhorar o uso da \textit{CPU}.  Dentro dos problemas que ele encontra está o excesso de copias de dados, a fragmentação de pacotes no \textit{socket}, o alinhamento do cache e desabilitar o \textit{netfilter} da ponte. Com essa mudança eles conseguiram uma economia de 56\% no uso do processador.

\cite{oi2009performance} analisou o desempenho de um sistema virtualizado com \textit{XEN} aplicando a estratégia \textit{LRO(large receive offload)}. Nesse experimento eles mediram a vazão da rede variando o tamanho da mensagem e o tamanho da MTU(unidade máxima de transmissão). Os resultados mostraram um ganho de 8\% a 14\% na vazão da rede.

\cite{xenbestpractices} sugere algumas práticas para tentar evitar um baixo desempenho em aplicações intensivas de rede/disco no XEN como dedicar exclusivamente um núcleo de processador ou uma quantidade de memória para o dom0 e dar mais tempo de processamento ao dom0.


\subsection{Análise das Informações}

\subsection{Conclusão}
