\chapter{Revisão Bibliográfica}
\label{cap:revisao_bibliografica}

\section{Objetivo}
O objetivo dessa revisão foi analisar e estudar as maneiras já existentes de otimizar o desempenho da rede em infraestruturas de máquinas virtuais utilizados para a criação de nuvens e os problemas em aberto. 
Cada estratégia sugerida pode atender bem a um cenário, porém, em outros casos, essa mesma estratégia pode ser pouco eficiente devido a dinamicidade da rede e os diferentes requerimentos de um usuário.

\section{Critérios de Seleção}
Para seleção das referências, em cada artigo encontrado pela estratégia de busca será lido o seu resumo e classificado manualmente em três categorias de acordo com sua relevância: alta, média, baixa.

Os artigos de relevância alta serão lidos por completo e resumidos. Os artigos médios terão a leitura de sua introdução e a mudança da sua relevância para baixa ou alta. Por fim os artigos baixos não serão lidos. 


\section{Execução}
{A tabela \ref{artigos} mostra os artigos coletados e sua relevância.

{
\begin{table}[ht!]
\caption{Artigos selecionados e suas relevâncias}
\label{artigos}
\begin{center}
	\begin{tabular}{| cp{2cm} | r}
		artigo&&relevância\\
	\hline
		\cite{chaudhary2008comparison}&&alta\\
		\cite{ekanayake2010high}&&alta\\
		\cite{Waldspurger:2012:IV:2063176.2063194}&&alta\\
		\cite{liu2010evaluating}&&alta\\
		\cite{Rixner:2008:NVB:1348583.1348592}&&alta\\
		\cite{shipman07:_inves_infin}&&média-baixa\\
		\cite{Santos:2008:BGS:1404014.1404017}&&alta\\ 
		\cite{oi2009performance}&&alta\\
		\cite{gro}&&alta\\
		\cite{Liao:2008:STI:1477942.1477971}&&média-baixa\\
		\cite{apparao2006characterization}&&alta\\
		\cite{jang2011low}&&alta\\
		\cite{fortuna2012improving}&&alta\\
		\cite{dong2011optimizing}&&alta\\
	\end{tabular}
\end{center}
\end{table}
}
}

\subsection{Resumo Sintetizado}
Em \cite{chaudhary2008comparison} o autor fez uma comparação entre diferentes tecnologias de virtualização.
Nos experimentos foi concluído que o \textit{hypervisor} XEN tem um desempenho baixo em termos de atraso na rede, porém alto em termos de largura de banda em relação a um ambiente com OpenVZ e um ambiente sem virtualização, enquanto que o OpenVZ tem uma perda considerável em largura de banda, mas um atraso pequeno. Os autores de não entram em detalhes sobre os motivos dos resultados terem sido esses.\newline

 \cite{ekanayake2010high} estudou a relação entre o número de núcleos e o número de MVs usando XEN e Eucalyptus como infraestrutura de nuvem. Foi concluído que a virtualização funciona bem para aplicações que não se comunicam muito,
 enquanto que em aplicações que são sensíveis a latência, houve uma perda de desempenho em relação a um ambiente não virtualizado. Outra conclusão foi que quanto maior o número de máquinas virtuais, maior a sobrecarga na CPU. A explicação para isso, segundo o autor, está na forma como foi implementada a virtualização da rede. O hardware físico só pode ser controlado por um sistema (\texttt{dom0}), enquanto que os outros (\texttt{domUs}) para conseguir fazer alguma operação de E/S pela rede, tem que passar por esse sistema através de um canal. Isso forma um gargalo no \texttt{dom0}.\newline

\cite{Rixner:2008:NVB:1348583.1348592} fez uma revisão sobre a virtualização de rede.
No texto o autor cita que a virtualização de rede impacta diretamente no número de servidores que podem ser diretamente consolidados dentro de uma única máquina física. Porém, as técnicas modernas de virtualização têm gargalos significantes, o que limita o desempenho da rede.
Ele sugere um ganho de desempenho fazendo o dispositivo ter a capacidade de ler e escrever diretamente na memória da MV ao invés do processador da máquina virtual gerar interrupções cada vez que alguma informação entra ou sai pelo dispositivo. Essa funcionalidade é chamada acesso direto a memória (\textit{DMA}).
Apesar disso, o dispositivo pode escrever em uma posição da memória que não pertence a MV e, podendo assim, causar problemas em outros processos da máquina física. Assim, foi criado a unidade de gerenciamento de E/S da memória (\textit{IOMMU}). No \textit{IOMMU} a memória é restrita para o dispositivo de acordo com a máquina virtual que controla esse dispositivo.
Como atualmente um processador possui vários núcleos, pode-se aproveitar esses núcleos para criar multi-filas nas interfaces de rede. O autor cita que pesquisadores do laboratório da HP e Citrix eliminaram a ponte no domínio de E/S para associar as máquinas virtuais diretamente com o \textit{driver} de \textit{backend} através das multi-filas, evitando a necessidade de sincronização das mensagens e multiplexação/demultiplexação da rede. Como benefícios do uso da multi-fila se teve: a redução da carga extra na fila e a eliminação de cópias entre o domínio de E/S e a máquina virtual, pois, a multiplexação não é feita. Por outro lado, seria necessário que cada informação seja enviada para a fila correta e que a CPU consiga aguentar a carga extra gerada pelas múltiplas filas.

Ainda em \cite{Rixner:2008:NVB:1348583.1348592}, na arquitetura de virtualização de rede CDNA (\textit{acesso direto a memória concorrente}) foi usada a ideia de multi-filas e em adição removeram o domínio de E/S. Sem o responsável por controlar as filas, o \textit{hypervisor} passa a considerar cada conjunto de fila como um interface de rede física e a associa o controlador a uma MV. Assim, cada MV consegue enviar ou receber informações diretamente da rede sem nenhuma intervensão do domínio de E/S. Como consequência, a carga extra é reduzida pelo número reduzido de interrupções (antes era necessário interromper tanto o domínio de E/S como as MVs em cada transmissão/recepção).
Pela MV poder acessar diretamente a interface de rede, ela também pode acessar algum local indevido da memória por \textit{DMA}. Para contornar esse problema o autor sugeriu o uso de \textit{IOMMU}.\newline

\cite{Waldspurger:2012:IV:2063176.2063194}  cita diversos desafios e problemas na área de virtualização de E/S: a carga extra no \textit{hypervisor}, a complexidade em gerenciar recursos (escalonamento e prioridades) e a dificuldade de dar uma semântica ao hardware virtual.

\cite{liu2010evaluating} fez diversos experimentos com virtualização de E/S baseada em \textit{software} (\textit{virtio}) e em hardware (\textit{SR-IOV}) usando o \textit{hypervisor} KVM.
O virtio é um padrão do linux para drivers de rede e disco que estão rodando em um ambiente virtual cooperado com um \textit{hypervisor}, apesar de diferentes, ele tem o mesmo padrão arquitetural que a virtualização de rede do XEN. 
Já o SR-IOV é uma especificação que permite dispositivos pci-Express fornecerem interfaces extras com funcionalidades reduzidas para serem usadas pelas máquinas virtuais diretamente.

Foram analisadas diversas métricas: a largura de banda, a latência, chamadas para memória e comunicação entre máquinas.

 Na latência, o \textit{virtio} teve um desempenho muito baixo. A explicação, provada desabilitando a função de mitigação de transmissão, é que o hospedeiro atrasa o envio do pacotes para ser enviado em rajadas, mas mesmo assim, seu desempenho sem mitigação ainda perdeu próximo de 20 microsegundo em relação a máquina pura.
Quando a opção de mitigação é desabilitada, isso provoca uma perda de desempenho pois cada pacote que é transmitido gera uma carga de trabalho no CPU, com a mitigação a carga por pacote é reduzida.  
Já o \textit{SR-IOV} (single root I/O virtualization) teve um desempenho próximo da máquina pura perdendo apenas alguns microssegundos devido a virtualização da interrupção.

 Na largura de banda, a transmissão em todos pareceu ter o mesmo desempenho. Já na recepção o \textit{SR-IOV}se aproximou da máquina pura, mas o uso da sua CPU foi muito maior que as demais. No \textit{virtio}, ele não conseguiu um bom desempenho, mas o uso de sua CPU foi baixa. No experimento de uso da memória na recepção, o \textit{SR-IOV} teve um uso muito menor que o \textit{virtio}, assim, o autor concluiu que o mal uso da largura de banda na recepção do virtio foi pelo uso excessivo da memória, o que explica também o baixo uso da CPU.

Nas transmissões de dados de MV, o \textit{SR-IOV} teve um ganho muito maior que o \textit{virtio}. A explicação do autor está na forma com que o \textit{virtio} age quando tem uma alta carga na recepção, durante esse momento, ele troca a interrupção para um modo de pesquisa que reduz o número de interrupções e melhora o desempenho da rede.
Na separação de custos, foi feita a separação do quanto foi utilizado de CPU para determinada tarefa. O resultado foi que o \textit{virtio} gasta uma quantidade maior na virtualização e cópia, enquanto que o \textit{SR-IOV} gasta uma quantidade maior em interrupções e nas máquinas virtuais.\newline

\cite{Santos:2008:BGS:1404014.1404017} propôs modificar a arquitetura do \textit{driver} de E/S do \textit{XEN} para conseguir melhorar o uso da \textit{CPU}.  Dentro dos problemas que ele encontra está o excesso de copias de dados, a fragmentação de pacotes no \textit{socket}, a falta de alinhamento do cache e o filtro de rede da ponte. Com algumas modificações ele conseguiu uma economia de 56\% no uso do processador.\newline

\cite{oi2009performance} analisou o desempenho de um sistema virtualizado com \textit{XEN} aplicando a estratégia \textit{LRO(large receive offload)} onde ainda dentro do \textit{driver} da placa de rede é recebido e reunido os pacotes de informações que tiveram que ser segmentados. Nesse experimento eles mediram a vazão da rede variando o tamanho da mensagem e o tamanho da \textit{MTU}(unidade máxima de transmissão). Os resultados mostraram um ganho de 8\% a 14\% na vazão da rede.\newline

\cite{xenbestpractices} sugere algumas práticas para tentar evitar um baixo desempenho em aplicações intensivas de rede/disco no XEN como dedicar exclusivamente um núcleo de processador ou uma quantidade de memória para o \texttt{dom0}.\newline

\cite{Liao:2008:STI:1477942.1477971} propôs duas otimizações na virtualização de rede: o escalonamento ciente de cache e o roubo de créditos de escalonamento para a recepção de pacotes. 
A primeira ideia é fazer com que o \texttt{domU} e o \texttt{dom0} passem a compartilhar o cache, assim, a comunicação entre domínios é reduzida.
A segunda otimização foca priorizar a recepção de pacotes onde o uso do processador é alto.
Nos experimentos comparando a estrutura padrão de virtualização e a estrutura modificada com as otimizações, foi apresentado um ganho de 96\% na largura de banda.\newline

\cite{apparao2006characterization} pesquisaram as principais causas de carga extra na virtualização de E/S. No experimento eles estudaram dois modos de virtualização de E/S: o \texttt{domU} e o \texttt{dom0} na mesma \textit{CPU} e em \textit{CPUs} distintas.
O resultado mostrou que nas duas, tanto a transmissão como a recepção de pacotes perderam mais de 50\% no desempenho comparado com a máquina física. Também foi notado que o rodar o \texttt{domU} e o \texttt{dom0} em \textit{CPUs} distintas é mais custoso que rodar elas juntas na mesma \textit{CPU}.\newline
 
\cite{jang2011low} estudaram sacrificar o isolamento que existe entre as máquinas virtuais para conseguir reduzir a carga extra do processo. Os resultados mostram uma redução de 29\% no uso do processador e 8\% de ganho de banda na transmissão de pacotes grandes. \newline

\cite{fortuna2012improving} fizeram experimentos em torno do problema da carga extra na virtualização da rede. Para isso eles propuseram adequar o balanço de interrupções para demostrar a possibilidade de reduzir o número de pacotes perdidos. O resultado foi que um balanço adequado pode melhorar muito o desempenho, porém, o comportamento é difícil de ser previsto, dificultando a elaboração de um algoritmo.

 Uma proposta futura sugerida foi deixar o núcleo do sistema automatizar o processo de balanço e analisar os resultados, quando aparecerem bons resultados, congelar a configuração de interrupção.

Eles também, no final, discutiram a possibilidade de usar a função de mesclagem existentes nos \textit{drivers} das placas de rede modernas.

\cite{dong2011optimizing} propuseram otimizações para reduzir a carga extra na virtualização da rede. Uma das otimizações foi mesclar eficientemente as interrupções virtuais e a outra escalar o lado da recepção. A mesclagem de interrupção normalmente é usada quando a transferência de pacotes do meio físico é muito alta. Com uma transferência grande a placa de rede passa a trabalhar intensamente sobrecarregando o processador com interrupções.
Na virtualização da rede, a transferência de pacotes passa a gerar interrupções físicas e virtuais. Interrupções virtuais são mais custosas em termos de ciclos de CPU em relação a interrupções físicas.

A mesclagem de interrupções virtuais pode ser feita no \textit{driver} virtual de \textit{backend}(na porte dentro do \texttt{dom0}) ou no \textit{driver} virtual de \textit{frontend}(dentro de um \texttt{domU}).

A ideia de escalar o lado da recepção foi baseado na ideia de paralelizar o \textit{driver} virtual de \textit{backend} tentando aproveitar melhor as propriedades de um processador multi-núcleo. Assim, foi introduzido o conceito de RSS que balanceia a carga de trabalho eficientemente entre os processadores.

O resultado no experimento de mesclagem de interrupções foi um ganho de até 76\% na largura de banda em relação a configuração padrão e na de escalar o lado da recepção foi até 2,2 vezes mais também na largura de banda.


\subsection{Análise das Informações}
Nessa revisão foram encontrados diversos artigos com propostas que modificam diferentes partes da infraestrutura: \textit{driver} de rede, placa de rede física, arquitetura da virtualização da rede e núcleo do sistema operacional.
Essa variação dificultou um pouco na correlação entre os artigos.

Nos experimentos todos parecem terem feito medições em infraestruturas reais usando o \textit{hypervisor} XEN e alguma distribuição linux como sistema operacional. 
Isso talvez ocorreu por eles terem o código aberto e o XEN suportar diferentes sistemas operacionais.

Todos que propuseram alguma estratégia as validaram através de medições em infraestruturas reais.
Uma possível causa seria a facilidade e o baixo custo em montar uma infraestrutura com virtualização e controlar todo o processo do experimento.

\subsection{Conclusão}
A revisão ajudou a entender melhor a área de virtualização de rede. Diversas formas de melhorar o desempenho foram encontradas. 
Numa próxima revisão, seria interessante focar em alguma categoria para facilitar a comparação entre pesquisas.
