\chapter{Revisão Bibliográfica}
\label{cap:revisao_bibliografica}

\section{Objetivo}
O objetivo dessa revisão foi analisar e estudar as maneiras já existentes de otimizar a utilização dos recursos da rede em infraestruturas de máquinas virtuais utilizadas para a criação de nuvens e os problemas em aberto. 
Cada estratégia sugerida pode atender bem a um cenário, porém, em outros casos, essa mesma estratégia pode ser pouco eficiente devido a dinamicidade da rede e os diferentes requisitos de \textit{QoS} de um usuário.

\section{Critérios de Seleção}
Para seleção das referências, em cada artigo encontrado foi lido o seu resumo e classificado manualmente em três categorias de acordo com sua relevância: alta, média, baixa.

Os artigos de relevância alta foram lidos por completo e resumidos. Os artigos de relevância média tiveram a leitura de sua introdução e a mudança da sua relevância para baixa ou alta. Por fim os artigos com relevância baixa não foram lidos. 
{A tabela \ref{artigos} mostra os artigos coletados e sua relevância.

{
\begin{table}[ht!]
\caption{Artigos selecionados e suas relevâncias}
\label{artigos}
\begin{center}
	\begin{tabular}{| cp{2cm} | r}
		referência&&relevância\\
	\hline
		\cite{chaudhary2008comparison}&&alta\\
		\cite{ekanayake2010high}&&alta\\
		\cite{Waldspurger:2012:IV:2063176.2063194}&&alta\\
		\cite{liu2010evaluating}&&alta\\
		\cite{Rixner:2008:NVB:1348583.1348592}&&alta\\
		\cite{shipman07:_inves_infin}&&média-baixa\\
		\cite{Santos:2008:BGS:1404014.1404017}&&alta\\ 
		\cite{oi2009performance}&&alta\\
		\cite{gro}&&alta\\
		\cite{Liao:2008:STI:1477942.1477971}&&média-alta\\
		\cite{apparao2006characterization}&&alta\\
		\cite{jang2011low}&&alta\\
		\cite{fortuna2012improving}&&alta\\
		\cite{dong2011optimizing}&&alta\\
	\end{tabular}
\end{center}
\end{table}
}
}

\subsection{Resumo Sintetizado}
Em \cite{chaudhary2008comparison}, o autor fez uma comparação entre o \textit{XEN}, \textit{VMWare} e \textit{OpenVZ}.
A partir dos experimentos foi concluído que o \textit{hypervisor} \textit{XEN} tem um desempenho baixo em termos de \sout{atraso na rede} \uline{latência}, porém alto em termos de largura de banda em relação a um ambiente com \textit{OpenVZ} e um ambiente sem virtualização, enquanto que o \textit{OpenVZ} tem uma perda em largura de banda, mas um atraso pequeno. Quanto ao \textit{VMWare}, ele teve um desempenho baixo tanto em atraso quanto em largura de banda. Os autores não entram em detalhes sobre os motivos dos resultados terem sido esses.\newline

Em \cite{ekanayake2010high}, foi estudado a relação entre o número de núcleos e o número de MVs usando \textit{XEN} e \textit{Eucalyptus} como infraestrutura de nuvem. Foi concluído que a virtualização funciona bem para aplicações que não se comunicam muito,
 enquanto que em aplicações que são sensíveis a latência, houve uma perda de desempenho em relação a um ambiente não virtualizado. Outra conclusão foi que quanto maior o número de máquinas virtuais, maior a sobrecarga na \textit{CPU}. A explicação para isso, segundo o autor, está na forma como é implementada a virtualização da rede. O hardware físico só pode ser controlado por um sistema (\texttt{dom0}), enquanto que os outros (\texttt{domUs}) para conseguirem fazer alguma operação de E/S pela rede, devem passar por esse sistema através de um canal. Isso forma um gargalo no \texttt{dom0}.\newline

Em \cite{Rixner:2008:NVB:1348583.1348592}, foi feita uma revisão sobre a virtualização de rede.
No texto o autor cita que a virtualização de rede impacta diretamente no número de servidores que podem ser diretamente consolidados dentro de uma única máquina física. Porém, as técnicas modernas de virtualização têm gargalos significantes, o que limita o desempenho da rede.
Ele sugere um ganho de desempenho fazendo o dispositivo ter a capacidade de ler e escrever diretamente na memória da MV ao invés do processador da máquina virtual gerar interrupções cada vez que alguma informação entra ou sai pelo dispositivo. Essa funcionalidade é chamada acesso direto a memória (\textit{DMA}).
Apesar disso, o dispositivo pode escrever em uma posição da memória que não pertence a MV, podendo assim, causar problemas em outros processos da máquina física. Assim, foi criado a unidade de gerenciamento de E/S da memória (\textit{IOMMU}). No \textit{IOMMU} a memória é restrita para o dispositivo de acordo com a máquina virtual que controla esse dispositivo. Atualmente os \textit{hypervisors} modernos, como o \textit{XEN}, utilizam essas técnicas \cite{barham2003xen} .

Como atualmente um processador possui vários núcleos, pode-se aproveitar esses núcleos para criar multi-filas nas interfaces de rede. O autor cita que pesquisadores do laboratório da HP e \textit{Citrix} eliminaram a ponte no domínio de E/S para associar as máquinas virtuais diretamente com o \textit{driver} de \textit{backend} através das multi-filas, evitando a necessidade de sincronização das mensagens e multiplexação/demultiplexação da rede. Como benefícios do uso da multi-fila se teve: a redução da carga extra na fila e a eliminação de cópias entre o domínio de E/S e a máquina virtual, pois, a multiplexação não é feita. Por outro lado, é necessário que cada informação seja enviada para a fila correta e que a \textit{CPU} consiga aguentar a carga extra gerada pelas múltiplas filas.

Ainda em \cite{Rixner:2008:NVB:1348583.1348592}, na arquitetura de virtualização de rede CDNA (\textit{acesso direto a memória concorrente}) foi empregada a proposta de multi-filas, em adição foi removido o domínio de E/S. Sem o responsável por controlar as filas, o \textit{hypervisor} passa a considerar cada conjunto de fila como uma interface de rede física e associa o controlador a uma MV. Assim, cada MV consegue enviar ou receber informações diretamente da rede sem nenhuma intervenção do domínio de E/S. Como consequência, a carga extra é reduzida pelo número reduzido de interrupções (antes era necessário interromper tanto o domínio de E/S como as MVs em cada transmissão/recepção).
Pela MV poder acessar diretamente a interface de rede, ela também pode acessar algum local indevido da memória por \textit{DMA}. Para contornar esse problema o autor sugeriu o uso de \textit{IOMMU}.\newline

Em \cite{Waldspurger:2012:IV:2063176.2063194}, são citados diversos desafios e problemas na área de virtualização de E/S: a carga extra no \textit{hypervisor}, a complexidade em gerenciar recursos (escalonamento e prioridades) e a dificuldade de dar uma semântica ao hardware virtual.\newline

Em \cite{liu2010evaluating}, foram feitos diversos experimentos com virtualização de E/S baseados em \textit{software} (\textit{virtio}) e em hardware (\textit{SR-IOV}) usando o \textit{hypervisor} \textit{KVM}.
O \textit{virtio} é um padrão do Linux para \textit{drivers} de rede e disco que estão rodando em um ambiente virtual cooperado com um \textit{hypervisor}. Apesar de diferentes, ele tem o mesmo padrão arquitetural que a virtualização de rede do \textit{XEN}. 
Já o \textit{SR-IOV} é uma especificação que permite a dispositivos \textit{pci-express} fornecerem interfaces extras com funcionalidades reduzidas para serem usadas pelas máquinas virtuais diretamente.

Foram analisadas diversas métricas: a largura de banda, a latência e uso do processador.
 Na latência, o \textit{virtio} teve um desempenho muito baixo. A explicação, provada desabilitando a função de agregação de interrupções na transmissão, é que o hospedeiro atrasa o envio do pacotes para ser enviado em rajadas, mas mesmo assim, seu desempenho sem mitigação ainda perdeu próximo de 20 microssegundo em relação a máquina não virtualizada.
Quando a opção de agregação é desabilitada, isso provoca uma perda de desempenho pois cada pacote que é transmitido gera uma carga de trabalho no \textit{CPU}. Com a mitigação a carga por pacote é reduzida.  
Já o \textit{SR-IOV} (single root I/O virtualization) teve um desempenho próximo da máquina pura perdendo apenas alguns microssegundos devido a virtualização da interrupção.

 Na largura de banda, a transmissão em todas as configurações pareceu ter o mesmo desempenho. Já na recepção o \textit{SR-IOV} se aproximou da máquina pura, mas o uso da sua \textit{CPU} foi muito maior que as demais. No \textit{virtio}, ele não conseguiu um bom desempenho, mas o uso de sua \textit{CPU} foi baixa. No experimento de uso da memória na recepção, o \textit{SR-IOV} teve um uso muito menor que o \textit{virtio}, assim, o autor concluiu que o mal uso da largura de banda na recepção do \textit{virtio} foi pelo uso excessivo da memória, o que explica também o baixo uso da \textit{CPU}.\newline

Em \cite{Santos:2008:BGS:1404014.1404017}, foi proposto modificar a arquitetura do \textit{driver} de E/S do \textit{XEN} para conseguir melhorar o uso da \textit{CPU}.  Dentro dos problemas que o autor encontrou está o excesso de cópias de dados.
\uline{
Para reduzir o excesso de cópias, foram propostos otimizações nas operações de remapeamento de páginas tanto na transmissão como na recepção. 
}
No \textit{hypervisor}, ele conseguiu uma economia de 56\% no uso do processador.\newline

Em \cite{oi2009performance}, foi analisado o desempenho de um sistema virtualizado com \textit{XEN} aplicando a estratégia \textit{LRO (large receive offload)} onde ainda dentro do \textit{driver} da placa de rede são recebidos e reunidos os pacotes de informações que tiveram que ser segmentados. Nesse experimento foram medidos a vazão da rede variando o tamanho da mensagem e o tamanho da \textit{MTU} (unidade máxima de transmissão). Os resultados mostraram um ganho de 8\% a 14\% na vazão da rede.\newline


Em \cite{Liao:2008:STI:1477942.1477971}, foram propostas duas otimizações na virtualização de rede: o escalonamento ciente do cache do processador e o roubo de créditos de escalonamento para a recepção de pacotes. 
A primeira ideia é fazer com que o \texttt{domU} e o \texttt{dom0} passem a compartilhar o cache, assim, a comunicação entre domínios é reduzida.
A segunda otimização foca priorizar a recepção de pacotes onde o uso do processador é alto.
Nos experimentos comparando a estrutura padrão de virtualização e a estrutura modificada com as otimizações, foi apresentado um aumento de 96\% na largura de banda.\newline

Em \cite{apparao2006characterization}, foi pesquisado as principais causas de carga extra na virtualização de E/S. No experimento eles estudaram dois modos de virtualização de E/S: o \texttt{domU} e o \texttt{dom0} na mesma \textit{CPU} e em \textit{CPUs} distintas.
O resultado mostrou que nos dois métodos, tanto a transmissão como a recepção de pacotes apresentaram uma perda de desempenho de mais de 50\% quando comparado com a máquina física. Também foi notado que rodar o \texttt{domU} e o \texttt{dom0} em \textit{CPUs} distintas é mais custoso que rodar elas juntas na mesma \textit{CPU}.\newline
 
Em \cite{jang2011low}, foi estudado sacrificar o isolamento que existe entre as máquinas virtuais para conseguir reduzir a carga extra do processador. Os resultados mostram uma redução de 29\% no uso do processador e 8\% de ganho de largura de banda na transmissão de pacotes grandes. \newline

Em \cite{fortuna2012improving}, foram feitos experimentos em torno do problema da carga extra na virtualização da rede. Para isso, os autores propuseram adequar o balanceamento de interrupções para demostrar a possibilidade de reduzir o número de pacotes perdidos. O resultado foi que um balanceamento adequado pode melhorar muito o desempenho, porém, o comportamento é difícil de ser previsto, dificultando a elaboração de um algoritmo.
 Uma proposta futura sugerida foi deixar o núcleo do sistema automatizar o processo de balanço e analisar os resultados. Quando aparecerem bons resultados, congelar a configuração de interrupção.

Eles também, discutiram a possibilidade de usar a função de agregação existentes nos \textit{drivers} das placas de rede modernas para um trabalho futuro.\newline

Em \cite{dong2011optimizing}, foram propostas otimizações para reduzir a carga extra na virtualização da rede. Uma das otimizações foi agregar eficientemente as interrupções virtuais e a outra escalar o lado da recepção. A agregação de interrupção normalmente é usada quando a transferência de pacotes do meio físico é muito alta. Com uma transferência grande, a placa de rede passa a trabalhar intensamente sobrecarregando o processador com interrupções.
Na virtualização da rede, a transferência de pacotes passa a gerar interrupções físicas e virtuais. 

A agregação de interrupções virtuais pode ser feita no \textit{driver} virtual de \textit{backend} (na ponte dentro do \texttt{dom0}) ou no \textit{driver} virtual de \textit{frontend} (dentro de um \texttt{domU}).

A ideia de escalar o lado da recepção foi baseado na ideia de paralelizar o \textit{driver} virtual de \textit{backend} tentando aproveitar melhor as propriedades de um processador multinúcleo. Assim, foi introduzido o conceito de \textit{RSS} que balanceia a carga de trabalho eficientemente entre os processadores.

O resultado no experimento de agregação de interrupções foi um ganho de até 76\% na largura de banda em relação a configuração padrão e no experimento de escalar o lado da recepção a largura de banda foi 2,2 vezes a largura de banda da configuração padrão.


\subsubsection{Evaluating System Performance in Gigabit Networks \cite{salah2005analysis}}

Em \cite{salah2005analysis}, é feita uma análise e simulação sobre o impacto da sobrecarga de interrupções no desempenho do sistema operacional em redes de alta velocidade. 
O principal problema que eles exploraram é a grande quantidade de interrupções gerada na recepção de pacotes. Como a interrupção tem prioridade máxima em relação a outras tarefas, ela acaba consumindo todo tempo de processamento, impedindo outras tarefas de serem realizadas. Isso reduz a taxa de transferência do sistema a 0. Essa situação é conhecida como \textit{``livelock''}.

Na Figura \ref{livelock}, é mostrado um gráfico de carga do sistema por taxa de transferência.
Na curva ideal, conforme a carga do sistema aumenta, a taxa de transferência passa a aumentar proporcionalmente, ou seja, quanto maior a velocidade de chegada dos pacotes, maior a quantidade de pacotes processada.
Porém, como praticamente todo sistema tem uma capacidade finita de processamento, ele não recebe e processa pacotes além de sua velocidade máxima. Essa velocidade é chamada de \textit{MLFRR} (do inglês \textit{Maximum Loss-Free Receive Rate} --  máxima velocidade de recepção livre de perdas).
Na curva aceitável, quando o sistema chega à \textit{MLFRR}, ele passa a não ter ganho na taxa de transferência pelo limite de processamento, e depois disso, a curva se comporta praticamente como uma constante.
Por outro lado, se a rede for sobrecarregada na entrada, as interrupções geradas na chegada dos pacotes irão impedir que o pacote seja processado e a taxa de transferência do sistema cairá para 0 como é possível ver na curva \textit{livelock}.


\begin{figure}[h!]
\begin{center}
  \includegraphics[width=300px,height=250px]{./img/livelock.eps}
\caption{Livelock na recepção de pacotes \cite{salah2005analysis}}
\label{livelock}
\end{center}
\end{figure}


Para analisar a situação de sobrecarga de interrupções, foi modelado o sistema como uma fila M/M/1/B com chegada de pacotes em \textit{Poisson} de velocidade $\lambda$ e média efetiva de tempo de serviço de $1/\mu$ que tem uma distribuição geral. A justificativa dos autores para usarem \textit{Poisson} na chegada de pacotes ao invés de rajadas, como o tráfego de pacotes normalmente se comportai, é a dificuldade de estudar, modelar e resolver analiticamente um sistema com esse tipo de tráfego.

O sistema pode usar ou não \textit{DMA}.
Sem \textit{DMA}, o processador gerencia a recepção de pacotes. Quando o processador é interrompido enquanto está processando um pacote pela chegada de um outro pacote, o tempo para processar é estendido para realizar uma cópia individual do outro pacote que chegou para a memória do sistema.
Com \textit{DMA}, a placa de rede tem acesso direto a memória. Quando um ou mais pacotes chegam enquanto que o sistema está ainda processando um outro pacote, todos são processados sem estender o tempo de processar. 

Foram feitos experimentos analisando o sistema ideal, \textit{DMA} habilitado e \textit{DMA} desabilitado. Com pouco tráfego de pacotes, a taxa de transferência de todos foi a mesma. 
Já com muito tráfego de pacotes, a taxa de transferência do sistema com o \textit{DMA} habilitado teve uma queda menor que o sistema sem o \textit{DMA} habilitado e o sistema ideal se apresentou com taxa de transferência constante.

\subsubsection{Interrupt Moderation Using Intel GbE Controllers \cite{intel_interrupt}}

Em \cite{intel_interrupt}, é citado o problema da grande quantidade de interrupções gerada na transmissão e recepção de pacotes.
 Para resolvê-lo, os autores propuseram o uso de temporizadores internos da placa de rede para moderar a quantidade de interrupções geradas.
 Os temporizadores são divididos em temporizador absoluto, pacote e mestre. O temporizador absoluto inicia uma contagem regressiva quando o primeiro pacote chega ou é enviado. 
No momento que a contagem chega a zero, é gerada uma interrupção no sistema. Este temporizador é eficiente quando se tem muito tráfego de pacotes, pois muitos pacotes chegam/são enviados até o temporizador gerar a interrupção, reduzindo a quantidade de interrupções por pacote. 
Por outro lado, ele não é eficiente quando há pouco tráfego porque poucos pacotes chegam/são enviados até o temporizador gerar a interrupção e por atrasar as interrupções e, consequentemente, as informações que devem chegar ao sistema.

O temporizador de pacotes também inicia uma contagem regressiva quando o primeiro pacote chega ou é enviado e também gera uma interrupção quando a contagem chega a zero, mas ele é reiniciado sempre que um novo pacote chega. 
Isso reduz a latência quando há pouco tráfego no enlace, pois a interrupção é gerada quando o temporizador percebe que nenhum pacote será mais enviado/recebido, mas quando há muito tráfego, ele pode nunca gerar a interrupção, pois o temporizador estará sempre sendo reinicializado pelos pacotes que chegam/são enviados.
O temporizador mestre é usado para otimizar os outros temporizadores.
O temporizador absoluto e o temporizador de pacotes podem ser combinados para chegar a um bom resultado.

Além dos temporizadores já citados, existe um outro mecanismo chamado limitador de interrupções. 
Esse mecanismo também é um temporizador de contagem regressiva e limita o número de interrupções por segundo. 
Quando o temporizador inicia a contagem regressiva, este também começa a contar o número de interrupções que foi gerado. 
Quando a contagem chega a zero, o contador de interrupções também é zerado. Se o número de interrupções ultrapassar o limite estabelecido, as interrupções geradas são adiadas até o contador ser reinicializado.

Um algoritmo foi proposto para moderação de interrupções que ajusta dinamicamente o valor do limitador de interrupções. 
Dependendo do padrão de E/S, é usado um valor no limitador. 
O padrão de E/S é categorizado em: baixíssima latência, onde o tráfego é mínimo e predomina os pacotes pequenos, baixa latência, onde o tráfego também é minimo e há um significante percentual de pacotes pequenos, e intermediário, onde há muito tráfego de pacotes medianos.
Não foi possível entender como os autores chegaram a esses valores e porque eles resolveram dividir o padrão de E/S dessa forma.



\subsubsection{To Coalesce or Not To Coalesce \cite{salah2007coalesce}}

Em \cite{salah2007coalesce}, continuação do artigo \cite{salah2005analysis}, é analisado o desempenho de duas técnicas de agregação de interrupções: baseada em contagem e baseada em tempo. Na técnica baseada em tempo, o  \textit{driver} da placa de rede não gera interrupções no sistema quando um pacote é recebido.
Ao invés disso, uma interrupção é gerada depois de um intervalo de tempo que um pacote é recebido.
Já na técnica baseada em contagem, é gerada uma interrupção  quando uma quantidade de pacotes é recebida. 

A conclusão tirada nos modelos analíticos é que a agregação funciona melhor que o modelo de interrupção comum quando se tem um grande tráfego na rede. Porém, para um tráfego pequeno, a interrupção comum superou a agregação. Os autores sugerem monitorar o tráfego e fazer a troca entre a interrupção comum e a agregação de interrupções. Eles também citam um momento que pode ser usado para indicar a condição de sobrecarga. Este momento pode ser usado para a troca. 
Outras importantes conclusões são que na agregação, são necessários valores altos de parâmetros em tráfegos intensivos, que para tráfegos tolerantes a latência, o uso de agregação é interessante independente do tráfego e que para tráfegos de tempo não-real é interessante usar a agregação baseada em tempo ao invés da baseada em contagem por impor um limite de atraso na agregação.

\subsection{Resumo Conclusivo}
Nessa revisão foram encontrados diversos artigos com propostas que modificam diferentes partes da infraestrutura: \textit{driver} de rede, placa de rede física, arquitetura da virtualização da rede e núcleo do sistema operacional.
Essa variação dificultou um pouco a correlação entre os artigos.

Nos experimentos todos parecem ter feito medições em infraestruturas reais usando o \textit{hypervisor} \textit{XEN} e alguma distribuição \textit{Linux} como sistema operacional. 
Isso talvez ocorreu por eles terem o código aberto e o \textit{XEN} suportar diferentes sistemas operacionais.

Todos que propuseram alguma estratégia as validaram através de medições em infraestruturas reais.
Uma possível causa seria a facilidade e o baixo custo em montar uma infraestrutura com virtualização e controlar todo o processo do experimento.

A revisão ajudou a entender melhor a área de virtualização de rede. Diversas formas de melhorar o desempenho foram encontradas. Porém, não foi encontrada nenhuma proposta de um algoritmo para agregar as interrupções que seja independente das tecnologias de nível mais baixo.
